{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by doing preproccesing!\n",
    "\n",
    "This works for both the the large csv and the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_2624\\2961142511.py:11: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Data/bigdata/big.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "womp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_2624\\2961142511.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['content'].replace(regex={'\\n{2,}': \"\" }, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitespace is gone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_2624\\2961142511.py:27: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_2624\\2961142511.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['content'].replace(regex={'https://[.a-zA-Z0-9/-]+|www[.a-zA-Z0-9/-]+|http://[.a-zA-Z0-9/-]+': '<URL>'}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLS gone\n",
      "EMAILS GONE\n",
      "DATE GONE\n",
      "NUM GONE\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "porter_stemmer = SnowballStemmer(\"english\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "df = pd.read_csv('Data/bigdata/big.csv')\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    stemmed_sentence = [porter_stemmer.stem(word) for word in token_words]\n",
    "    return \" \".join(stemmed_sentence)\n",
    "\n",
    "if type(df['content']) == str:\n",
    "    df['content'] = df['content'].apply(stem_sentence)\n",
    "else:\n",
    "    print(\"womp\")\n",
    "\n",
    "df['content'].replace(regex={'\\n{2,}': \"\" }, inplace=True)\n",
    "df['content'].replace(regex={'\\t{2,}': \"\" }, inplace=True)\n",
    "df['content'].replace(regex={'\\r{2,}': \"\" }, inplace=True)\n",
    "print(\"whitespace is gone\")\n",
    "df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "print(\"lowercase\")\n",
    "df['content'].replace(regex={'https://[.a-zA-Z0-9/-]+|www[.a-zA-Z0-9/-]+|http://[.a-zA-Z0-9/-]+': '<URL>'}, inplace=True)\n",
    "print(\"URLS gone\")\n",
    "df['content'].replace(regex={'[a-z]+@[a-z]+.[a-z]+': '<EMAIL>'}, inplace=True)\n",
    "print(\"EMAILS GONE\")\n",
    "df['content'].replace(regex={'([0-9]{2})[/]?[-]?([0-9]{2})[/]?[-]?([0-9]{4})|([0-9]{4})[/]?[-]?([0-9]{2})[/]?[-]?([0-9]{2})': '<DATE>'}, inplace=True)\n",
    "print(\"DATE GONE\")\n",
    "df['content'].replace(regex={'[0-9]+': '<NUM>'}, inplace=True)\n",
    "print(\"NUM GONE\")\n",
    "df.to_csv('Data/otherdata/SOMETHING.csv', index=True)\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also quickly want to group the labels of the dataset, so that there are only 2 labels one for realnews and one for fakenews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_2624\\4220074590.py:4: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Data/otherdata/SOMETHING.csv')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV file with specified data types\n",
    "df = pd.read_csv('Data/otherdata/SOMETHING.csv')\n",
    "\n",
    "# Define a function to map types to categories\n",
    "def map_type(Thetype):\n",
    "    if Thetype in ['fake', 'bias', 'hate', 'conspiracy', 'junksci', 'satire', 'state']:\n",
    "        return 'FakeNews'\n",
    "    elif Thetype in ['reliable', 'political']:\n",
    "        return 'RealNews'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['type'] = df['type'].apply(map_type)\n",
    "\n",
    "something = df.drop(df[df['type'].isnull()].index)\n",
    "LastSomething = something.drop(something[something['content'].isnull()].index)\n",
    "\n",
    "LastSomething.to_csv('Data/otherdata/BIGCLEANED.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a preproccesed dataset we can now start building our base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (F1score)0.34847852579072813\n",
      "Accuracy: 0.5289296230672157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['basemodel.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/BIGCLEANED.csv')\n",
    "\n",
    "def count_words(text):\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "    return len(numbers)\n",
    "\n",
    "def calculate_length(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return len(obj)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "X = df['content'].apply(calculate_length)\n",
    "Y = df[\"type\"].dropna()  \n",
    "\n",
    "X_train, X_combined_test, y_train, y_combined_test = train_test_split(X, Y, test_size=0.2, random_state=32)\n",
    "\n",
    "\n",
    "X_test1, X_test2, y_test1, y_test2 = train_test_split(X_combined_test, y_combined_test, test_size=0.5, random_state=32 )\n",
    "basemodel = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "basemodel.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = basemodel.predict(X_test1.values.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "f1score = f1_score(y_test1, y_pred, average='macro')\n",
    "print(f'Accuracy (F1score){f1score}')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "dump(basemodel, 'basemodel.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add meta-data to the model, so we will be traning a new model with the title meta-data, the resoning is shown in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to do this we also need to remove all the articles with missing title values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pandas as pd\\n\\n# Load data from CSV file with specified data types\\ndf = pd.read_csv('Data/otherdata/BIGCLEANED.csv')\\n\\n# Define a function to map types to categories\\ndef map_type(Thetype):\\n    if Thetype in ['fake', 'bias', 'hate', 'conspiracy', 'junksci', 'satire', 'state']:\\n        return 'FakeNews'\\n    elif Thetype in ['reliable', 'political']:\\n        return 'RealNews'\\n    elif Thetype in ['RealNews', 'FakeNews']:\\n        return Thetype\\n    else:\\n        return None\\n\\ndf['type'] = df['type'].apply(map_type)\\n\\nsomething = df.drop(df[df['type'].isnull()].index)\\nLastSomething = something.drop(something[something['content'].isnull()].index)\\nLastLastSomething = LastSomething.drop(LastSomething[LastSomething['title'].isnull()].index) \\nLastLastSomething.to_csv('Data/otherdata/Trainedwithtittle.csv', index=False)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV file with specified data types\n",
    "df = pd.read_csv('Data/otherdata/BIGCLEANED.csv')\n",
    "\n",
    "# Define a function to map types to categories\n",
    "def map_type(Thetype):\n",
    "    if Thetype in ['fake', 'bias', 'hate', 'conspiracy', 'junksci', 'satire', 'state']:\n",
    "        return 'FakeNews'\n",
    "    elif Thetype in ['reliable', 'political']:\n",
    "        return 'RealNews'\n",
    "    elif Thetype in ['RealNews', 'FakeNews']:\n",
    "        return Thetype\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['type'] = df['type'].apply(map_type)\n",
    "\n",
    "something = df.drop(df[df['type'].isnull()].index)\n",
    "LastSomething = something.drop(something[something['content'].isnull()].index)\n",
    "LastLastSomething = LastSomething.drop(LastSomething[LastSomething['title'].isnull()].index) \n",
    "LastLastSomething.to_csv('Data/otherdata/Trainedwithtittle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/otherdata/Trainedwithtittle.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/otherdata/Trainedwithtittle.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_words\u001b[39m(text):\n\u001b[0;32m     15\u001b[0m     numbers \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)?\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/otherdata/Trainedwithtittle.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/Trainedwithtittle.csv')\n",
    "\n",
    "def count_words(text):\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "    return len(numbers)\n",
    "\n",
    "def calculate_length(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return len(obj)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "X = df['content'].apply(count_words) + df['title'].apply(count_words)\n",
    "Y = df[\"type\"].dropna()  \n",
    "\n",
    "X_train, X_combined_test, y_train, y_combined_test = train_test_split(X, Y, test_size=0.2, random_state=32)\n",
    "\n",
    "\n",
    "X_test1, X_test2, y_test1, y_test2 = train_test_split(X_combined_test, y_combined_test, test_size=0.5, random_state=32 )\n",
    "basemodel = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "basemodel.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = basemodel.predict(X_test1.values.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "f1score = f1_score(y_test1, y_pred, average='macro')\n",
    "print(f'Accuracy (F1score){f1score}')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "dump(basemodel, 'basemodelwithtitle.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained that we will also look at the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we look at the results of the simple model without meta-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL WITHOUT TITLE\n",
      "Accuracy (F1score)0.3453329962775426\n",
      "Accuracy (Not f1score)0.5273234840469605\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import collections, numpy\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "def count_words(text):\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "    return len(numbers)\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/BIGCLEANED.csv')\n",
    "X = df[\"content\"].dropna().apply(count_words)\n",
    "y = df[\"type\"].dropna()\n",
    "X = X[:len(y)]\n",
    "\n",
    "# Split the data into one training set and one combined test set\n",
    "X_train, X_combined_test, y_train, y_combined_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "# Split the combined test set into two separate test sets\n",
    "X_test1, X_test2, y_test1, y_test2 = train_test_split(X_combined_test, y_combined_test, test_size=0.5, random_state=32 )\n",
    "\n",
    "loaded_model = load('basemodel.joblib')\n",
    "loaded_model.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(X_test2.values.reshape(-1, 1))\n",
    "\n",
    "print(\"BASE MODEL WITHOUT TITLE\")\n",
    "# Print the predictions\n",
    "f1score = f1_score(y_test2, predictions, average='macro')\n",
    "test_accuracy = accuracy_score(y_test2, predictions)\n",
    "print(f'Accuracy (F1score){f1score}')\n",
    "print(f'Accuracy (Not f1score){test_accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the simple model using meta-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL WITH TITLE\n",
      "Accuracy (F1score)0.5144203342063822\n",
      "Accuracy (Not f1score)0.5782792483954945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import collections, numpy\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "def count_words(text):\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "    return len(numbers)\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/Trainedwithtittle.csv')\n",
    "X = df[\"content\"].dropna().apply(count_words) + df[\"title\"].dropna().apply(count_words)\n",
    "y = df[\"type\"].dropna()  \n",
    "X = X[:len(y)]\n",
    "\n",
    "# Split the data into one training set and one combined test set\n",
    "X_train, X_combined_test, y_train, y_combined_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "# Split the combined test set into two separate test sets\n",
    "X_test1, X_test2, y_test1, y_test2 = train_test_split(X_combined_test, y_combined_test, test_size=0.5, random_state=32 )\n",
    "\n",
    "loaded_model = load('basemodel.joblib')\n",
    "loaded_model.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(X_test2.values.reshape(-1, 1))\n",
    "\n",
    "print(\"BASE MODEL WITH TITLE\")\n",
    "# Print the predictions\n",
    "f1score = f1_score(y_test2, predictions, average='macro')\n",
    "test_accuracy = accuracy_score(y_test2, predictions)\n",
    "print(f'Accuracy (F1score){f1score}')\n",
    "print(f'Accuracy (Not f1score){test_accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try add data we scapred from BBC and see how this updates the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to clean the BBC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_22040\\2463135151.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['text'].replace(regex={'\\n{2,}': \"\" }, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitespace is gone\n",
      "lowercase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_22040\\2463135151.py:23: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_22040\\2463135151.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['text'].replace(regex={'https://[.a-zA-Z0-9/-]+|www[.a-zA-Z0-9/-]+|http://[.a-zA-Z0-9/-]+': '<URL>'}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLS gone\n",
      "EMAILS GONE\n",
      "DATE GONE\n",
      "NUM GONE\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "porter_stemmer = SnowballStemmer(\"english\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/BBC.csv')\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    stemmed_sentence = [porter_stemmer.stem(word) for word in token_words]\n",
    "    return \" \".join(stemmed_sentence)\n",
    "\n",
    "df['text'].replace(regex={'\\n{2,}': \"\" }, inplace=True)\n",
    "df['text'].replace(regex={'\\t{2,}': \"\" }, inplace=True)\n",
    "df['text'].replace(regex={'\\r{2,}': \"\" }, inplace=True)\n",
    "print(\"whitespace is gone\")\n",
    "df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "print(\"lowercase\")\n",
    "df['text'].replace(regex={'https://[.a-zA-Z0-9/-]+|www[.a-zA-Z0-9/-]+|http://[.a-zA-Z0-9/-]+': '<URL>'}, inplace=True)\n",
    "print(\"URLS gone\")\n",
    "df['text'].replace(regex={'[a-z]+@[a-z]+.[a-z]+': '<EMAIL>'}, inplace=True)\n",
    "print(\"EMAILS GONE\")\n",
    "df['text'].replace(regex={'([0-9]{2})[/]?[-]?([0-9]{2})[/]?[-]?([0-9]{4})|([0-9]{4})[/]?[-]?([0-9]{2})[/]?[-]?([0-9]{2})': '<DATE>'}, inplace=True)\n",
    "print(\"DATE GONE\")\n",
    "df['text'].replace(regex={'[0-9]+': '<NUM>'}, inplace=True)\n",
    "print(\"NUM GONE\")\n",
    "df.to_csv('Data/otherdata/CleanBBC.csv', index=True)\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to combine the BBC data with out dataset with title. There after we want to clean the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/Trainedwithtittle.csv')\n",
    "gf = pd.read_csv('Data/otherdata/CleanBBC.csv')\n",
    "\n",
    "gf = gf.rename(columns={'text': 'Content', 'headline': 'title'})\n",
    "gf['type'] = 'RealNews'\n",
    "\n",
    "selected_columns = gf[['Content', 'title', 'type']]\n",
    "\n",
    "combined_df = pd.concat([df, selected_columns], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('Data/otherdata/Combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to preprocess the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_22040\\480460179.py:4: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Data/otherdata/Combined.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV file with specified data types\n",
    "df = pd.read_csv('Data/otherdata/Combined.csv')\n",
    "\n",
    "# Define a function to map types to categories\n",
    "def map_type(Thetype):\n",
    "    if Thetype in ['fake', 'bias', 'hate', 'conspiracy', 'junksci', 'satire', 'state']:\n",
    "        return 'FakeNews'\n",
    "    elif Thetype in ['reliable', 'political']:\n",
    "        return 'RealNews'\n",
    "    elif Thetype in ['FakeNews','RealNews']:\n",
    "        return Thetype\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['type'] = df['type'].apply(map_type)\n",
    "\n",
    "something = df.drop(df[df['type'].isnull()].index)\n",
    "LastSomething = something.drop(something[something['content'].isnull()].index)\n",
    "LastLastSomething = LastSomething.drop(LastSomething[LastSomething['title'].isnull()].index) #Use when traning with tittle! \n",
    "LastSomething.to_csv('Data/otherdata/CleanedCombined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/otherdata/CleanedCombined.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/otherdata/CleanedCombined.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_words\u001b[39m(text):\n\u001b[0;32m     15\u001b[0m     numbers \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)?\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\First\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/otherdata/CleanedCombined.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/CleanedCombined.csv')\n",
    "\n",
    "def count_words(text):\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "    return len(numbers)\n",
    "\n",
    "def calculate_length(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return len(obj)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "X = df['content'].apply(count_words) + df['title'].apply(count_words)\n",
    "\n",
    "Y = df[\"type\"].dropna()  \n",
    "\n",
    "X_train, X_combined_test, y_train, y_combined_test = train_test_split(X, Y, test_size=0.2, random_state=32)\n",
    "\n",
    "\n",
    "X_test1, X_test2, y_test1, y_test2 = train_test_split(X_combined_test, y_combined_test, test_size=0.5, random_state=32 )\n",
    "basemodel = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "basemodel.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = basemodel.predict(X_test1.values.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "f1score = classification_report(y_test1, y_pred, average='macro')\n",
    "print(f'Accuracy (F1score){f1score}')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "dump(basemodel, 'basemodelBBC.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a advanced model, we will use sk-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784489\n",
      "784489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/BIGCLEANED.csv')\n",
    "X = df[\"content\"].dropna()\n",
    "Y = df[\"type\"].dropna()  \n",
    "Y = Y[:len(X)]\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.2, random_state=32)\n",
    "\n",
    "X_test, X_REALTEST, y_test, y_REALTEST = train_test_split(X_temp, y_temp, test_size=0.5, random_state=32)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n",
    "model.fit(X_train_vectors, y_train)\n",
    "\n",
    "\n",
    "dump(model, 'model.joblib')\n",
    "dump(vectorizer, 'vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will also be trying to make a model using meta-data, once again it will be using title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775933\n",
      "775933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Advancedvectorizerwithtitle.joblib']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/Trainedwithtittle.csv')\n",
    "X = df[\"content\"].dropna() + df[\"title\"].dropna()\n",
    "Y = df[\"type\"].dropna()  \n",
    "Y = Y[:len(X)]\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.2, random_state=32)\n",
    "\n",
    "X_test, X_REALTEST, y_test, y_REALTEST = train_test_split(X_temp, y_temp, test_size=0.5, random_state=32)\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n",
    "model.fit(X_train_vectors, y_train)\n",
    "\n",
    "\n",
    "dump(model, 'Advancedmodelwithtitle.joblib')\n",
    "dump(vectorizer, 'Advancedvectorizerwithtitle.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see the results of the models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the model without the title meta-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADVANCED MODEL WITHOUT TITLE\n",
      "F1score: 0.8907844667053688\n",
      "Accuracy: 0.8911139721347627\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import collections, numpy\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/BIGCLEANED.csv')\n",
    "\n",
    "X = df[\"content\"].dropna()\n",
    "Y = df[\"type\"].dropna()  \n",
    "\n",
    "X = X[:len(Y)]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.2, random_state=32)\n",
    "\n",
    "X_test, X_REALTEST, y_test, y_REALTEST = train_test_split(X_temp, y_temp, test_size=0.5, random_state=32)\n",
    "\n",
    "vectorizer = load('vectorizer.joblib')\n",
    "loaded_model = load('model.joblib')\n",
    "\n",
    "# Vectorize the input data using the loaded vectorizer\n",
    "New_test_vectors = vectorizer.transform(X_REALTEST)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(New_test_vectors)\n",
    "\n",
    "print(\"ADVANCED MODEL WITHOUT TITLE\")\n",
    "# Print the predictions\n",
    "f1score = f1_score(y_REALTEST, predictions, average='macro')\n",
    "test_accuracy = accuracy_score(y_REALTEST, predictions)\n",
    "print(f'F1score: {f1score}')\n",
    "print(f'Accuracy: {test_accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the meta-data title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775933\n",
      "775933\n",
      "ADVANCED MODEL WITH TITLE\n",
      "F1score: 0.8964756875459738\n",
      "Accuracy: 0.8969250199757713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import collections, numpy\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv('Data/otherdata/Trainedwithtittle.csv')\n",
    "X = df[\"content\"].dropna() + df[\"title\"].dropna()\n",
    "Y = df[\"type\"].dropna()  \n",
    "Y = Y[:len(X)]\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.2, random_state=32)\n",
    "\n",
    "X_test, X_REALTEST, y_test, y_REALTEST = train_test_split(X_temp, y_temp, test_size=0.5, random_state=32)\n",
    "\n",
    "vectorizer = load('Advancedvectorizerwithtitle.joblib')\n",
    "loaded_model = load('Advancedmodelwithtitle.joblib')\n",
    "\n",
    "# Vectorize the input data using the loaded vectorizer\n",
    "New_test_vectors = vectorizer.transform(X_REALTEST)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(New_test_vectors)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"ADVANCED MODEL WITH TITLE\")\n",
    "f1score = f1_score(y_REALTEST, predictions, average='macro')\n",
    "test_accuracy = accuracy_score(y_REALTEST, predictions)\n",
    "print(f'F1score: {f1score}')\n",
    "print(f'Accuracy: {test_accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we will test our models on the Liar dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we firstly need to prepair the dataset by labeling the types and then we are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/liardata/test.tsv', header=None, delimiter='\\t')\n",
    "\n",
    "# Define a function to map types to categories\n",
    "def map_type(Thetype):\n",
    "    if str(Thetype) in ['false', 'pants-fire', 'barely-true']:\n",
    "        return 'FakeNews'\n",
    "    if str(Thetype) in ['mostly-true', 'true']:\n",
    "        return 'RealNews'\n",
    "    else:\n",
    "        return None  # Return None for types that don't match the conditions\n",
    "\n",
    "df[1] = df[1].apply(map_type)\n",
    "\n",
    "newting = df.drop(df[df[1].isnull()].index)\n",
    "\n",
    "# Save the modified DataFrame to a new TSV file\n",
    "newting.to_csv('Data/liardata/FinalTest.tsv', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test our models on the Liar dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with testing the advanced model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (F1score)0.5279398801786861\n",
      "Accuracy (Not f1score)0.5279441117764471\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import collections, numpy\n",
    "from sklearn.metrics import f1_score\n",
    "df = pd.read_csv('Data/liardata/FinalTest.tsv', sep='\\t' , header=None)\n",
    "#Load data from tsv test file\n",
    "content = df.iloc[:, 2]\n",
    "Realawnser = df.iloc[:, 1]\n",
    "#Load models\n",
    "vectorizer = load('vectorizer.joblib')\n",
    "loaded_model = load('model.joblib')\n",
    "#Transform content\n",
    "New_test_vectors = vectorizer.transform(content)\n",
    "predictions = loaded_model.predict(New_test_vectors)\n",
    "f1score = f1_score(Realawnser, predictions, average='macro')\n",
    "test_accuracy = accuracy_score(Realawnser, predictions)\n",
    "print(f'Accuracy (F1score){f1score}')\n",
    "print(f'Accuracy (Not f1score){test_accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (F1score)0.35562700964630223\n",
      "Accuracy (Not f1score)0.5518962075848304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import load, dump\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections, numpy\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv('Data/liardata/FinalTest.tsv', sep='\\t' , header=None)\n",
    "\n",
    "def count_words(text):\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "    return len(numbers)\n",
    "\n",
    "#Load data from tsv test file\n",
    "content = df.iloc[:, 2].apply(count_words)\n",
    "Realawnser = df.iloc[:, 1]\n",
    "\n",
    "loaded_model = load('basemodel.joblib')\n",
    "\n",
    "content_reshaped = np.array(content).reshape(-1, 1)\n",
    "predictions = loaded_model.predict(content_reshaped)\n",
    "f1score = f1_score(Realawnser, predictions, average='macro')\n",
    "test_accuracy = accuracy_score(Realawnser, predictions)\n",
    "print(f'Accuracy (F1score){f1score}')\n",
    "print(f'Accuracy (Not f1score){test_accuracy}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "First",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
